/**
 * RL Loop (AgentEvolver) - PoC
 * 
 * æ¼”ç¤ºå¦‚ä½•é€šè¿‡è‡ªæˆ‘æé—®å’Œç»†ç²’åº¦å½’å› å®ç°å¼ºåŒ–å­¦ä¹ 
 */

import * as fs from 'fs';
import * as path from 'path';

// ============================================================================
// ç±»å‹å®šä¹‰
// ============================================================================

interface State {
  id: string;
  taskDescription: string;
  codebaseContext: string;
  history: string[];
  features: Record<string, number>;
}

interface Action {
  id: string;
  type: 'search' | 'analyze' | 'modify' | 'test' | 'commit';
  description: string;
  parameters: Record<string, any>;
  confidence: number;
}

interface Trajectory {
  id: string;
  taskId: string;
  states: State[];
  actions: Action[];
  rewards: number[];
  outcome: {
    success: boolean;
    codeQuality: number;
    timeTaken: number;
    errorCount: number;
  };
  timestamp: Date;
}

interface Attribution {
  actionId: string;
  contribution: number;  // -1 åˆ° 1
  confidence: number;
  reason: string;
  features: Record<string, number>;
}

interface RewardComponents {
  taskSuccess: number;
  codeQuality: number;
  efficiency: number;
  safety: number;
  exploration: number;
}

interface SelfQuestion {
  id: string;
  question: string;
  type: 'exploration' | 'verification' | 'reflection';
  context: string;
  expectedAnswer?: string;
}

// ============================================================================
// Self Questioner
// ============================================================================

class SelfQuestioner {
  private questionTemplates = {
    exploration: [
      'What if I try a different approach for {context}?',
      'Are there alternative tools I could use here?',
      'What would happen if I searched in a different directory?',
      'Could this problem be solved more efficiently?'
    ],
    verification: [
      'Does this solution handle all edge cases?',
      'Have I considered the error paths?',
      'Will this change break existing functionality?',
      'Is this the minimal necessary change?'
    ],
    reflection: [
      'Why did the previous approach fail?',
      'What was the key insight that led to success?',
      'What assumptions did I make that were wrong?',
      'What would I do differently next time?'
    ]
  };

  /**
   * ç”Ÿæˆè‡ªæˆ‘æé—®
   */
  generateQuestions(state: State, recentAction?: Action): SelfQuestion[] {
    const questions: SelfQuestion[] = [];

    // åŸºäºå½“å‰çŠ¶æ€ç”Ÿæˆæ¢ç´¢æ€§é—®é¢˜
    questions.push({
      id: `q_${Date.now()}_exp`,
      question: this.selectTemplate('exploration', state.taskDescription),
      type: 'exploration',
      context: state.taskDescription
    });

    // å¦‚æœæœ‰æœ€è¿‘çš„è¡ŒåŠ¨ï¼Œç”ŸæˆéªŒè¯æ€§é—®é¢˜
    if (recentAction) {
      questions.push({
        id: `q_${Date.now()}_ver`,
        question: this.selectTemplate('verification', recentAction.description),
        type: 'verification',
        context: recentAction.description
      });
    }

    // å¦‚æœæœ‰å†å²ï¼Œç”Ÿæˆåæ€æ€§é—®é¢˜
    if (state.history.length > 0) {
      const lastFailure = state.history.filter(h => h.includes('failed')).slice(-1)[0];
      if (lastFailure) {
        questions.push({
          id: `q_${Date.now()}_ref`,
          question: this.selectTemplate('reflection', lastFailure),
          type: 'reflection',
          context: lastFailure
        });
      }
    }

    return questions;
  }

  private selectTemplate(type: 'exploration' | 'verification' | 'reflection', context: string): string {
    const templates = this.questionTemplates[type];
    const template = templates[Math.floor(Math.random() * templates.length)];
    return template.replace('{context}', context.slice(0, 50));
  }
}

// ============================================================================
// Attribution Analyzer
// ============================================================================

class AttributionAnalyzer {
  /**
   * åˆ†æè½¨è¿¹ä¸­æ¯ä¸ªåŠ¨ä½œçš„è´¡çŒ®
   */
  analyze(trajectory: Trajectory): Attribution[] {
    const attributions: Attribution[] = [];

    // 1. è®¡ç®—æ€»å¥–åŠ±
    const totalReward = trajectory.rewards.reduce((a, b) => a + b, 0);

    // 2. ä¸ºæ¯ä¸ªåŠ¨ä½œè®¡ç®—è´¡çŒ®
    for (let i = 0; i < trajectory.actions.length; i++) {
      const action = trajectory.actions[i];
      const reward = trajectory.rewards[i] || 0;

      // åäº‹å®æ¨ç†ï¼šå¦‚æœæ²¡æœ‰è¿™ä¸ªåŠ¨ä½œä¼šæ€æ ·ï¼Ÿ
      const counterfactualReward = this.simulateCounterfactual(trajectory, i);

      // è´¡çŒ®åº¦ = å®é™…å¥–åŠ± - åäº‹å®å¥–åŠ±
      const contribution = (reward - counterfactualReward) / Math.max(Math.abs(totalReward), 1);

      // ç¡®å®šè´¡çŒ®åŸå› 
      let reason = 'Neutral contribution';
      if (contribution > 0.1) {
        reason = 'Positive impact on task completion';
      } else if (contribution < -0.1) {
        reason = 'Negative impact or wasted effort';
      }

      attributions.push({
        actionId: action.id,
        contribution: Math.max(-1, Math.min(1, contribution)),
        confidence: 0.7,  // ç®€åŒ–çš„ç½®ä¿¡åº¦
        reason,
        features: this.extractActionFeatures(action)
      });
    }

    return attributions;
  }

  /**
   * æ¨¡æ‹Ÿåäº‹å®ï¼ˆç®€åŒ–ç‰ˆï¼‰
   */
  private simulateCounterfactual(trajectory: Trajectory, actionIndex: number): number {
    // ç®€åŒ–çš„åäº‹å®æ¨¡æ‹Ÿ
    // å®é™…ä¸­åº”è¯¥é‡æ–°è¿è¡Œä»»åŠ¡æˆ–ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹

    const rewardsWithout = [...trajectory.rewards];
    rewardsWithout[actionIndex] = 0;

    // å‡è®¾åç»­å¥–åŠ±ä¼šå‡å°‘ï¼ˆå› ä¸ºè¿™ä¸ªåŠ¨ä½œæ²¡æœ‰å‘ç”Ÿï¼‰
    for (let i = actionIndex + 1; i < rewardsWithout.length; i++) {
      rewardsWithout[i] *= 0.8;  // 20% è¡°å‡
    }

    return rewardsWithout.reduce((a, b) => a + b, 0) / trajectory.rewards.length;
  }

  /**
   * æå–åŠ¨ä½œç‰¹å¾
   */
  private extractActionFeatures(action: Action): Record<string, number> {
    return {
      is_search: action.type === 'search' ? 1 : 0,
      is_modify: action.type === 'modify' ? 1 : 0,
      is_test: action.type === 'test' ? 1 : 0,
      confidence: action.confidence
    };
  }
}

// ============================================================================
// Reward Calculator
// ============================================================================

class RewardCalculator {
  private weights = {
    taskSuccess: 10.0,
    codeQuality: 2.0,
    efficiency: 1.0,
    safety: -2.0,
    exploration: 0.5
  };

  /**
   * è®¡ç®—å¥–åŠ±
   */
  calculate(outcome: Trajectory['outcome'], action: Action): number {
    let reward = 0;

    // 1. ä»»åŠ¡æˆåŠŸå¥–åŠ±
    if (outcome.success) {
      reward += this.weights.taskSuccess;
    }

    // 2. ä»£ç è´¨é‡å¥–åŠ±
    reward += outcome.codeQuality * this.weights.codeQuality;

    // 3. æ•ˆç‡å¥–åŠ±
    const efficiencyBonus = Math.max(0, 1 - outcome.timeTaken / 300);  // 5 åˆ†é’ŸåŸºå‡†
    reward += efficiencyBonus * this.weights.efficiency;

    // 4. é”™è¯¯æƒ©ç½š
    reward -= outcome.errorCount * 0.5;

    // 5. æ¢ç´¢å¥–åŠ±ï¼ˆé¼“åŠ±æ¢ç´¢æ€§è¡Œä¸ºï¼‰
    if (action.type === 'search' || action.confidence < 0.7) {
      reward += this.weights.exploration;
    }

    return reward;
  }

  /**
   * åˆ†è§£å¥–åŠ±ç»„ä»¶
   */
  breakdown(outcome: Trajectory['outcome']): RewardComponents {
    return {
      taskSuccess: outcome.success ? this.weights.taskSuccess : 0,
      codeQuality: outcome.codeQuality * this.weights.codeQuality,
      efficiency: Math.max(0, 1 - outcome.timeTaken / 300) * this.weights.efficiency,
      safety: 0,  // æ— å®‰å…¨é—®é¢˜æ—¶ä¸º 0
      exploration: 0.5  // åŸºç¡€æ¢ç´¢å¥–åŠ±
    };
  }
}

// ============================================================================
// Policy Optimizer
// ============================================================================

class PolicyOptimizer {
  private learningRate = 0.001;
  private epsilon = 0.1;  // æ¢ç´¢ç‡
  private policy: Map<string, number[]> = new Map();  // çŠ¶æ€ -> åŠ¨ä½œæ¦‚ç‡

  /**
   * é€‰æ‹©åŠ¨ä½œ
   */
  selectAction(state: State, availableActions: Action[]): Action {
    // Îµ-greedy ç­–ç•¥
    if (Math.random() < this.epsilon) {
      // æ¢ç´¢ï¼šéšæœºé€‰æ‹©
      return availableActions[Math.floor(Math.random() * availableActions.length)];
    }

    // åˆ©ç”¨ï¼šé€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
    return this.bestAction(state, availableActions);
  }

  /**
   * é€‰æ‹©æœ€ä¼˜åŠ¨ä½œ
   */
  private bestAction(state: State, availableActions: Action[]): Action {
    // è®¡ç®—æ¯ä¸ªåŠ¨ä½œçš„æœŸæœ›å€¼
    let bestAction = availableActions[0];
    let bestValue = -Infinity;

    for (const action of availableActions) {
      const value = this.estimateActionValue(state, action);
      if (value > bestValue) {
        bestValue = value;
        bestAction = action;
      }
    }

    return bestAction;
  }

  /**
   * ä¼°è®¡åŠ¨ä½œä»·å€¼
   */
  private estimateActionValue(state: State, action: Action): number {
    // åŸºäºç‰¹å¾çš„çŠ¶æ€-åŠ¨ä½œå€¼ä¼°è®¡
    const stateKey = this.getStateKey(state);

    if (!this.policy.has(stateKey)) {
      // åˆå§‹åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ
      this.policy.set(stateKey, [0.5, 0.5, 0.5, 0.5, 0.5]);
    }

    const actionProbs = this.policy.get(stateKey)!;
    const actionIndex = this.getActionIndex(action.type);

    return actionProbs[actionIndex] * action.confidence;
  }

  /**
   * æ›´æ–°ç­–ç•¥
   */
  updatePolicy(attributions: Attribution[], trajectory: Trajectory): void {
    for (const attr of attributions) {
      const action = trajectory.actions.find(a => a.id === attr.actionId);
      if (!action) continue;

      const state = trajectory.states[trajectory.actions.indexOf(action)];
      const stateKey = this.getStateKey(state);
      const actionIndex = this.getActionIndex(action.type);

      if (!this.policy.has(stateKey)) {
        this.policy.set(stateKey, [0.5, 0.5, 0.5, 0.5, 0.5]);
      }

      const probs = this.policy.get(stateKey)!;

      // ç®€å•çš„ç­–ç•¥æ¢¯åº¦æ›´æ–°
      const update = this.learningRate * attr.contribution;
      probs[actionIndex] = Math.max(0, Math.min(1, probs[actionIndex] + update));

      this.policy.set(stateKey, probs);
    }
  }

  private getStateKey(state: State): string {
    // ç®€åŒ–çš„çŠ¶æ€é”®
    return state.taskDescription.slice(0, 20).replace(/\s+/g, '_');
  }

  private getActionIndex(actionType: Action['type']): number {
    const indices = { search: 0, analyze: 1, modify: 2, test: 3, commit: 4 };
    return indices[actionType] || 0;
  }

  /**
   * è·å–å½“å‰ç­–ç•¥
   */
  getPolicy(): Record<string, number[]> {
    return Object.fromEntries(this.policy);
  }
}

// ============================================================================
// RL Environment
// ============================================================================

class RLEnvironment {
  private currentTask: string = '';
  private stepCount: number = 0;
  private maxSteps: number = 20;

  /**
   * é‡ç½®ç¯å¢ƒ
   */
  reset(taskDescription: string): State {
    this.currentTask = taskDescription;
    this.stepCount = 0;

    return {
      id: `state_${Date.now()}`,
      taskDescription,
      codebaseContext: 'Loading codebase...',
      history: [],
      features: {}
    };
  }

  /**
   * æ‰§è¡ŒåŠ¨ä½œ
   */
  async step(action: Action): Promise<{
    state: State;
    reward: number;
    done: boolean;
    info: any;
  }> {
    this.stepCount++;

    // æ¨¡æ‹ŸåŠ¨ä½œæ‰§è¡Œ
    const newState: State = {
      id: `state_${Date.now()}`,
      taskDescription: this.currentTask,
      codebaseContext: 'Updated context',
      history: [`Action: ${action.description}`],
      features: {}
    };

    // ç®€åŒ–çš„å¥–åŠ±è®¡ç®—
    const reward = action.confidence * 0.5;
    const done = this.stepCount >= this.maxSteps;

    return {
      state: newState,
      reward,
      done,
      info: {
        step: this.stepCount,
        actionType: action.type
      }
    };
  }
}

// ============================================================================
// Agent Trainer
// ============================================================================

class AgentTrainer {
  private questioner: SelfQuestioner;
  private attributionAnalyzer: AttributionAnalyzer;
  private rewardCalculator: RewardCalculator;
  private policyOptimizer: PolicyOptimizer;
  private environment: RLEnvironment;

  private trajectories: Trajectory[] = [];
  private storagePath: string;

  constructor(storagePath: string = './rl-storage') {
    this.questioner = new SelfQuestioner();
    this.attributionAnalyzer = new AttributionAnalyzer();
    this.rewardCalculator = new RewardCalculator();
    this.policyOptimizer = new PolicyOptimizer();
    this.environment = new RLEnvironment();
    this.storagePath = storagePath;
  }

  /**
   * è®­ç»ƒä¸€ä¸ªå›åˆ
   */
  async trainEpisode(taskDescription: string): Promise<{
    trajectory: Trajectory;
    attributions: Attribution[];
    totalReward: number;
  }> {
    console.log(`\nğŸ¯ Starting training episode: ${taskDescription.slice(0, 50)}...`);

    const trajectory: Trajectory = {
      id: `traj_${Date.now()}`,
      taskId: `task_${Date.now()}`,
      states: [],
      actions: [],
      rewards: [],
      outcome: {
        success: false,
        codeQuality: 0.7,
        timeTaken: 0,
        errorCount: 0
      },
      timestamp: new Date()
    };

    let state = this.environment.reset(taskDescription);
    trajectory.states.push(state);
    const startTime = Date.now();

    while (true) {
      // 1. ç”Ÿæˆè‡ªæˆ‘æé—®
      const lastAction = trajectory.actions.slice(-1)[0];
      const questions = this.questioner.generateQuestions(state, lastAction);

      if (questions.length > 0) {
        console.log(`   Question: ${questions[0].question}`);
      }

      // 2. ç”Ÿæˆå¯ç”¨åŠ¨ä½œ
      const availableActions = this.generateActions(state);

      // 3. é€‰æ‹©åŠ¨ä½œ
      const action = this.policyOptimizer.selectAction(state, availableActions);
      console.log(`   Action: ${action.type} - ${action.description.slice(0, 30)}...`);

      // 4. æ‰§è¡ŒåŠ¨ä½œ
      const stepResult = await this.environment.step(action);

      // 5. è®°å½•
      trajectory.actions.push(action);
      trajectory.rewards.push(stepResult.reward);
      trajectory.states.push(stepResult.state);

      // 6. æ£€æŸ¥æ˜¯å¦ç»“æŸ
      if (stepResult.done) {
        trajectory.outcome.success = stepResult.reward > 5;
        trajectory.outcome.timeTaken = (Date.now() - startTime) / 1000;
        break;
      }

      state = stepResult.state;
    }

    // 7. å½’å› åˆ†æ
    const attributions = this.attributionAnalyzer.analyze(trajectory);
    console.log(`\n   Attributions:`);
    attributions.slice(0, 3).forEach(attr => {
      console.log(`   - ${attr.actionId}: ${attr.contribution.toFixed(2)} (${attr.reason})`);
    });

    // 8. ç­–ç•¥æ›´æ–°
    this.policyOptimizer.updatePolicy(attributions, trajectory);

    // 9. ä¿å­˜è½¨è¿¹
    this.trajectories.push(trajectory);
    this.saveTrajectory(trajectory);

    const totalReward = trajectory.rewards.reduce((a, b) => a + b, 0);
    console.log(`\n   Total reward: ${totalReward.toFixed(2)}`);
    console.log(`   Success: ${trajectory.outcome.success ? 'âœ…' : 'âŒ'}`);

    return { trajectory, attributions, totalReward };
  }

  /**
   * ç”Ÿæˆå¯ç”¨åŠ¨ä½œ
   */
  private generateActions(state: State): Action[] {
    return [
      {
        id: `act_${Date.now()}_1`,
        type: 'search',
        description: 'Search for relevant code in the codebase',
        parameters: { query: state.taskDescription },
        confidence: 0.8
      },
      {
        id: `act_${Date.now()}_2`,
        type: 'analyze',
        description: 'Analyze the found code structure',
        parameters: {},
        confidence: 0.7
      },
      {
        id: `act_${Date.now()}_3`,
        type: 'modify',
        description: 'Apply code modification',
        parameters: {},
        confidence: 0.6
      },
      {
        id: `act_${Date.now()}_4`,
        type: 'test',
        description: 'Run tests to verify changes',
        parameters: {},
        confidence: 0.9
      },
      {
        id: `act_${Date.now()}_5`,
        type: 'commit',
        description: 'Commit the changes',
        parameters: {},
        confidence: 0.85
      }
    ];
  }

  /**
   * ä¿å­˜è½¨è¿¹
   */
  private saveTrajectory(trajectory: Trajectory): void {
    if (!fs.existsSync(this.storagePath)) {
      fs.mkdirSync(this.storagePath, { recursive: true });
    }
    const filePath = path.join(this.storagePath, `trajectory_${trajectory.id}.json`);
    fs.writeFileSync(filePath, JSON.stringify(trajectory, null, 2));
  }

  /**
   * è·å–è®­ç»ƒç»Ÿè®¡
   */
  getStats(): {
    totalEpisodes: number;
    avgReward: number;
    successRate: number;
  } {
    const totalEpisodes = this.trajectories.length;
    const avgReward = this.trajectories.length > 0
      ? this.trajectories.reduce((sum, t) => sum + t.rewards.reduce((a, b) => a + b, 0), 0) / totalEpisodes
      : 0;
    const successCount = this.trajectories.filter(t => t.outcome.success).length;
    const successRate = totalEpisodes > 0 ? successCount / totalEpisodes : 0;

    return { totalEpisodes, avgReward, successRate };
  }

  /**
   * è·å–å½“å‰ç­–ç•¥
   */
  getPolicy(): Record<string, number[]> {
    return this.policyOptimizer.getPolicy();
  }
}

// ============================================================================
// Demo
// ============================================================================

async function runDemo() {
  console.log('='.repeat(60));
  console.log('RL Loop (AgentEvolver) - PoC Demo');
  console.log('='.repeat(60));
  console.log();

  const trainer = new AgentTrainer('./rl-storage');

  // è®­ç»ƒå¤šä¸ªå›åˆ
  const tasks = [
    'Fix the null pointer exception in the user service',
    'Optimize the database query performance',
    'Add error handling to the API endpoint'
  ];

  for (let i = 0; i < tasks.length; i++) {
    console.log(`\n${'='.repeat(60)}`);
    console.log(`Episode ${i + 1}/${tasks.length}`);
    console.log('='.repeat(60));

    await trainer.trainEpisode(tasks[i]);

    // æ˜¾ç¤ºç»Ÿè®¡
    const stats = trainer.getStats();
    console.log(`\nğŸ“Š Current Stats:`);
    console.log(`   Episodes: ${stats.totalEpisodes}`);
    console.log(`   Avg Reward: ${stats.avgReward.toFixed(2)}`);
    console.log(`   Success Rate: ${(stats.successRate * 100).toFixed(1)}%`);
  }

  // æ˜¾ç¤ºæœ€ç»ˆç­–ç•¥
  console.log(`\n${'='.repeat(60)}`);
  console.log('Final Policy (sample):');
  console.log('='.repeat(60));
  const policy = trainer.getPolicy();
  Object.entries(policy).slice(0, 3).forEach(([state, probs]) => {
    console.log(`   ${state}: [${probs.map(p => p.toFixed(2)).join(', ')}]`);
  });

  console.log(`\n${'='.repeat(60)}`);
  console.log('Demo completed!');
  console.log('='.repeat(60));
}

// è¿è¡Œ Demo
if (require.main === module) {
  runDemo().catch(console.error);
}

export {
  AgentTrainer,
  SelfQuestioner,
  AttributionAnalyzer,
  RewardCalculator,
  PolicyOptimizer,
  RLEnvironment,
  State,
  Action,
  Trajectory,
  Attribution,
  SelfQuestion
};
